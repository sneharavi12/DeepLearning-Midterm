{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/5635.0 images\n",
      "Done: 100/5635.0 images\n",
      "Done: 200/5635.0 images\n",
      "Done: 300/5635.0 images\n",
      "Done: 400/5635.0 images\n",
      "Done: 500/5635.0 images\n",
      "Done: 600/5635.0 images\n",
      "Done: 700/5635.0 images\n",
      "Done: 800/5635.0 images\n",
      "Done: 900/5635.0 images\n",
      "Done: 1000/5635.0 images\n",
      "Done: 1100/5635.0 images\n",
      "Done: 1200/5635.0 images\n",
      "Done: 1300/5635.0 images\n",
      "Done: 1400/5635.0 images\n",
      "Done: 1500/5635.0 images\n",
      "Done: 1600/5635.0 images\n",
      "Done: 1700/5635.0 images\n",
      "Done: 1800/5635.0 images\n",
      "Done: 1900/5635.0 images\n",
      "Done: 2000/5635.0 images\n",
      "Done: 2100/5635.0 images\n",
      "Done: 2200/5635.0 images\n",
      "Done: 2300/5635.0 images\n",
      "Done: 2400/5635.0 images\n",
      "Done: 2500/5635.0 images\n",
      "Done: 2600/5635.0 images\n",
      "Done: 2700/5635.0 images\n",
      "Done: 2800/5635.0 images\n",
      "Done: 2900/5635.0 images\n",
      "Done: 3000/5635.0 images\n",
      "Done: 3100/5635.0 images\n",
      "Done: 3200/5635.0 images\n",
      "Done: 3300/5635.0 images\n",
      "Done: 3400/5635.0 images\n",
      "Done: 3500/5635.0 images\n",
      "Done: 3600/5635.0 images\n",
      "Done: 3700/5635.0 images\n",
      "Done: 3800/5635.0 images\n",
      "Done: 3900/5635.0 images\n",
      "Done: 4000/5635.0 images\n",
      "Done: 4100/5635.0 images\n",
      "Done: 4200/5635.0 images\n",
      "Done: 4300/5635.0 images\n",
      "Done: 4400/5635.0 images\n",
      "Done: 4500/5635.0 images\n",
      "Done: 4600/5635.0 images\n",
      "Done: 4700/5635.0 images\n",
      "Done: 4800/5635.0 images\n",
      "Done: 4900/5635.0 images\n",
      "Done: 5000/5635.0 images\n",
      "Done: 5100/5635.0 images\n",
      "Done: 5200/5635.0 images\n",
      "Done: 5300/5635.0 images\n",
      "Done: 5400/5635.0 images\n",
      "Done: 5500/5635.0 images\n",
      "Done: 5600/5635.0 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "Done: 0/5508 images\n",
      "Done: 100/5508 images\n",
      "Done: 200/5508 images\n",
      "Done: 300/5508 images\n",
      "Done: 400/5508 images\n",
      "Done: 500/5508 images\n",
      "Done: 600/5508 images\n",
      "Done: 700/5508 images\n",
      "Done: 800/5508 images\n",
      "Done: 900/5508 images\n",
      "Done: 1000/5508 images\n",
      "Done: 1100/5508 images\n",
      "Done: 1200/5508 images\n",
      "Done: 1300/5508 images\n",
      "Done: 1400/5508 images\n",
      "Done: 1500/5508 images\n",
      "Done: 1600/5508 images\n",
      "Done: 1700/5508 images\n",
      "Done: 1800/5508 images\n",
      "Done: 1900/5508 images\n",
      "Done: 2000/5508 images\n",
      "Done: 2100/5508 images\n",
      "Done: 2200/5508 images\n",
      "Done: 2300/5508 images\n",
      "Done: 2400/5508 images\n",
      "Done: 2500/5508 images\n",
      "Done: 2600/5508 images\n",
      "Done: 2700/5508 images\n",
      "Done: 2800/5508 images\n",
      "Done: 2900/5508 images\n",
      "Done: 3000/5508 images\n",
      "Done: 3100/5508 images\n",
      "Done: 3200/5508 images\n",
      "Done: 3300/5508 images\n",
      "Done: 3400/5508 images\n",
      "Done: 3500/5508 images\n",
      "Done: 3600/5508 images\n",
      "Done: 3700/5508 images\n",
      "Done: 3800/5508 images\n",
      "Done: 3900/5508 images\n",
      "Done: 4000/5508 images\n",
      "Done: 4100/5508 images\n",
      "Done: 4200/5508 images\n",
      "Done: 4300/5508 images\n",
      "Done: 4400/5508 images\n",
      "Done: 4500/5508 images\n",
      "Done: 4600/5508 images\n",
      "Done: 4700/5508 images\n",
      "Done: 4800/5508 images\n",
      "Done: 4900/5508 images\n",
      "Done: 5000/5508 images\n",
      "Done: 5100/5508 images\n",
      "Done: 5200/5508 images\n",
      "Done: 5300/5508 images\n",
      "Done: 5400/5508 images\n",
      "Done: 5500/5508 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "data_path = '/home/ubuntu/Notebooks/raw'\n",
    "\n",
    "image_rows = 420\n",
    "image_cols = 580\n",
    "\n",
    "\n",
    "def create_train_data():\n",
    "\n",
    "    train_data_path = os.path.join(data_path, 'train')\n",
    "    images = os.listdir(train_data_path)\n",
    "    #images = images[:1001]\n",
    "    total = len(images) / 2\n",
    "\n",
    "    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.tif'\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_train.npy', imgs)\n",
    "    np.save('imgs_mask_train.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_train = imgs_train[:1000]\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    imgs_mask_train = imgs_mask_train[:1000]\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "\n",
    "def create_test_data():\n",
    "    train_data_path = os.path.join(data_path, 'test')\n",
    "    images = os.listdir(train_data_path)\n",
    "    #images = images[:500]\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_id = np.ndarray((total, ), dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img_id = int(image_name.split('.')[0])\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = np.array([img])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_id[i] = img_id\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_test.npy', imgs)\n",
    "    np.save('imgs_id_test.npy', imgs_id)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_test = imgs_test[:300]\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    imgs_id = imgs_id[:300]\n",
    "    return imgs_test, imgs_id\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_train_data()\n",
    "    create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_train = imgs_train[:1000]\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    imgs_mask_train = imgs_mask_train[:1000]\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_test = imgs_test[:300]\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    imgs_id = imgs_id[:300]\n",
    "    return imgs_test, imgs_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and create the model \n",
    "from keras import optimizers\n",
    "from keras.layers import Activation, Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,CSVLogger\n",
    "\n",
    "def get_unet():\n",
    "    from keras.utils.layer_utils import print_summary\n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    dropout1 = Dropout(0.25)(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(dropout1)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2),dim_ordering='th')(conv1)\n",
    "    #imshow(l.activation(inpic)) \n",
    "\n",
    "    #dropout2 = Dropout(0.25)(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv2)\n",
    "\n",
    "    #dropout3 = Dropout(0.25)(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv3)\n",
    "\n",
    "    #dropout4 = Dropout(0.25)(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv4)\n",
    "\n",
    "    #dropout5 = Dropout(0.25)(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid',dim_ordering='th')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    #model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    #rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #metrics={'outbin': 'accuracy'}\n",
    "\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[dice_coef])\n",
    "\n",
    "    print_summary(model.layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,CSVLogger\n",
    "\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], imgs.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i, 0] = cv2.resize(imgs[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "    return imgs_p\n",
    "\n",
    "\n",
    "def train_and_predict():\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "\n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "    print('-'*30)\n",
    "    print('Shape')\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet()\n",
    "    model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\n",
    "    csv_logger = CSVLogger('training_unet.log')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=32, nb_epoch=100, verbose=1, shuffle=True,\n",
    "              callbacks=[model_checkpoint,csv_logger], validation_split=0.25)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing test data...')\n",
    "    print('-'*30)\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('unet.hdf5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "    np.save('imgs_mask_test_unet.npy', imgs_mask_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('tf')  # Tensor Flow dimension ordering in this code\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Shape\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 1, 64, 80)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 1, 64, 80)     0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_39 (Convolution2D) (None, 32, 64, 80)    320         dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_40 (Convolution2D) (None, 32, 64, 80)    9248        convolution2d_39[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 32, 32, 40)    0           convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_41 (Convolution2D) (None, 64, 32, 40)    18496       maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_42 (Convolution2D) (None, 64, 32, 40)    36928       convolution2d_41[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 64, 16, 20)    0           convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_43 (Convolution2D) (None, 128, 16, 20)   73856       maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_44 (Convolution2D) (None, 128, 16, 20)   147584      convolution2d_43[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_11 (MaxPooling2D)   (None, 128, 8, 10)    0           convolution2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_45 (Convolution2D) (None, 256, 8, 10)    295168      maxpooling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_46 (Convolution2D) (None, 256, 8, 10)    590080      convolution2d_45[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_12 (MaxPooling2D)   (None, 256, 4, 5)     0           convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_47 (Convolution2D) (None, 512, 4, 5)     1180160     maxpooling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_48 (Convolution2D) (None, 512, 4, 5)     2359808     convolution2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_9 (UpSampling2D)    (None, 512, 8, 10)    0           convolution2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_9 (Merge)                  (None, 768, 8, 10)    0           upsampling2d_9[0][0]             \n",
      "                                                                   convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_49 (Convolution2D) (None, 256, 8, 10)    1769728     merge_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_50 (Convolution2D) (None, 256, 8, 10)    590080      convolution2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_10 (UpSampling2D)   (None, 256, 16, 20)   0           convolution2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_10 (Merge)                 (None, 384, 16, 20)   0           upsampling2d_10[0][0]            \n",
      "                                                                   convolution2d_44[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_51 (Convolution2D) (None, 128, 16, 20)   442496      merge_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_52 (Convolution2D) (None, 128, 16, 20)   147584      convolution2d_51[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_11 (UpSampling2D)   (None, 128, 32, 40)   0           convolution2d_52[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_11 (Merge)                 (None, 192, 32, 40)   0           upsampling2d_11[0][0]            \n",
      "                                                                   convolution2d_42[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_53 (Convolution2D) (None, 64, 32, 40)    110656      merge_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_54 (Convolution2D) (None, 64, 32, 40)    36928       convolution2d_53[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_12 (UpSampling2D)   (None, 64, 64, 80)    0           convolution2d_54[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_12 (Merge)                 (None, 96, 64, 80)    0           upsampling2d_12[0][0]            \n",
      "                                                                   convolution2d_40[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_55 (Convolution2D) (None, 32, 64, 80)    27680       merge_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_56 (Convolution2D) (None, 32, 64, 80)    9248        convolution2d_55[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_57 (Convolution2D) (None, 1, 64, 80)     33          convolution2d_56[0][0]           \n",
      "====================================================================================================\n",
      "Total params: 7846081\n",
      "____________________________________________________________________________________________________\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 750 samples, validate on 250 samples\n",
      "Epoch 1/100\n",
      "750/750 [==============================] - 355s - loss: -0.0251 - dice_coef: 0.0251 - val_loss: -0.0211 - val_dice_coef: 0.0211\n",
      "Epoch 2/100\n",
      "750/750 [==============================] - 353s - loss: -0.0260 - dice_coef: 0.0260 - val_loss: -0.0226 - val_dice_coef: 0.0226\n",
      "Epoch 3/100\n",
      "750/750 [==============================] - 354s - loss: -0.0315 - dice_coef: 0.0315 - val_loss: -0.0316 - val_dice_coef: 0.0316\n",
      "Epoch 4/100\n",
      "750/750 [==============================] - 357s - loss: -0.0654 - dice_coef: 0.0654 - val_loss: -0.0999 - val_dice_coef: 0.0999\n",
      "Epoch 5/100\n",
      "750/750 [==============================] - 355s - loss: -0.1948 - dice_coef: 0.1948 - val_loss: -0.2016 - val_dice_coef: 0.2016\n",
      "Epoch 6/100\n",
      "750/750 [==============================] - 354s - loss: -0.2233 - dice_coef: 0.2233 - val_loss: -0.1868 - val_dice_coef: 0.1868\n",
      "Epoch 7/100\n",
      "750/750 [==============================] - 355s - loss: -0.2343 - dice_coef: 0.2343 - val_loss: -0.2173 - val_dice_coef: 0.2173\n",
      "Epoch 8/100\n",
      "750/750 [==============================] - 360s - loss: -0.2225 - dice_coef: 0.2225 - val_loss: -0.1755 - val_dice_coef: 0.1755\n",
      "Epoch 9/100\n",
      "750/750 [==============================] - 372s - loss: -0.2388 - dice_coef: 0.2388 - val_loss: -0.2215 - val_dice_coef: 0.2215\n",
      "Epoch 10/100\n",
      "750/750 [==============================] - 393s - loss: -0.2511 - dice_coef: 0.2511 - val_loss: -0.2210 - val_dice_coef: 0.2210\n",
      "Epoch 11/100\n",
      "750/750 [==============================] - 407s - loss: -0.2518 - dice_coef: 0.2518 - val_loss: -0.2186 - val_dice_coef: 0.2186\n",
      "Epoch 12/100\n",
      "750/750 [==============================] - 429s - loss: -0.2536 - dice_coef: 0.2536 - val_loss: -0.2155 - val_dice_coef: 0.2155\n",
      "Epoch 13/100\n",
      "750/750 [==============================] - 432s - loss: -0.2583 - dice_coef: 0.2583 - val_loss: -0.2295 - val_dice_coef: 0.2295\n",
      "Epoch 14/100\n",
      "750/750 [==============================] - 435s - loss: -0.2628 - dice_coef: 0.2628 - val_loss: -0.2324 - val_dice_coef: 0.2324\n",
      "Epoch 15/100\n",
      "750/750 [==============================] - 417s - loss: -0.2176 - dice_coef: 0.2176 - val_loss: -0.1842 - val_dice_coef: 0.1842\n",
      "Epoch 16/100\n",
      "750/750 [==============================] - 416s - loss: -0.2272 - dice_coef: 0.2272 - val_loss: -0.2017 - val_dice_coef: 0.2017\n",
      "Epoch 17/100\n",
      "750/750 [==============================] - 423s - loss: -0.2608 - dice_coef: 0.2608 - val_loss: -0.2380 - val_dice_coef: 0.2380\n",
      "Epoch 18/100\n",
      "750/750 [==============================] - 409s - loss: -0.2635 - dice_coef: 0.2635 - val_loss: -0.2464 - val_dice_coef: 0.2464\n",
      "Epoch 19/100\n",
      "750/750 [==============================] - 410s - loss: -0.2687 - dice_coef: 0.2687 - val_loss: -0.2413 - val_dice_coef: 0.2413\n",
      "Epoch 20/100\n",
      "750/750 [==============================] - 396s - loss: -0.2792 - dice_coef: 0.2792 - val_loss: -0.2459 - val_dice_coef: 0.2459\n",
      "Epoch 21/100\n",
      "750/750 [==============================] - 389s - loss: -0.2898 - dice_coef: 0.2898 - val_loss: -0.2682 - val_dice_coef: 0.2682\n",
      "Epoch 22/100\n",
      "750/750 [==============================] - 371s - loss: -0.2763 - dice_coef: 0.2763 - val_loss: -0.2643 - val_dice_coef: 0.2643\n",
      "Epoch 23/100\n",
      "750/750 [==============================] - 367s - loss: -0.2996 - dice_coef: 0.2996 - val_loss: -0.2797 - val_dice_coef: 0.2797\n",
      "Epoch 24/100\n",
      "750/750 [==============================] - 362s - loss: -0.2926 - dice_coef: 0.2926 - val_loss: -0.2291 - val_dice_coef: 0.2291\n",
      "Epoch 25/100\n",
      "750/750 [==============================] - 360s - loss: -0.3046 - dice_coef: 0.3046 - val_loss: -0.2984 - val_dice_coef: 0.2984\n",
      "Epoch 26/100\n",
      "750/750 [==============================] - 359s - loss: -0.3314 - dice_coef: 0.3314 - val_loss: -0.3054 - val_dice_coef: 0.3054\n",
      "Epoch 27/100\n",
      "750/750 [==============================] - 357s - loss: -0.3462 - dice_coef: 0.3462 - val_loss: -0.3196 - val_dice_coef: 0.3196\n",
      "Epoch 28/100\n",
      "750/750 [==============================] - 357s - loss: -0.3548 - dice_coef: 0.3548 - val_loss: -0.3254 - val_dice_coef: 0.3254\n",
      "Epoch 29/100\n",
      "750/750 [==============================] - 356s - loss: -0.3771 - dice_coef: 0.3771 - val_loss: -0.3310 - val_dice_coef: 0.3310\n",
      "Epoch 30/100\n",
      "750/750 [==============================] - 356s - loss: -0.3786 - dice_coef: 0.3786 - val_loss: -0.3345 - val_dice_coef: 0.3345\n",
      "Epoch 31/100\n",
      "750/750 [==============================] - 375s - loss: -0.4810 - dice_coef: 0.4810 - val_loss: -0.4149 - val_dice_coef: 0.4149\n",
      "Epoch 44/100\n",
      "750/750 [==============================] - 373s - loss: -0.4715 - dice_coef: 0.4715 - val_loss: -0.4091 - val_dice_coef: 0.4091\n",
      "Epoch 45/100\n",
      "750/750 [==============================] - 381s - loss: -0.4713 - dice_coef: 0.4713 - val_loss: -0.4051 - val_dice_coef: 0.4051\n",
      "Epoch 46/100\n",
      "750/750 [==============================] - 378s - loss: -0.4753 - dice_coef: 0.4753 - val_loss: -0.4312 - val_dice_coef: 0.4312\n",
      "Epoch 47/100\n",
      "750/750 [==============================] - 381s - loss: -0.5012 - dice_coef: 0.5012 - val_loss: -0.4151 - val_dice_coef: 0.4151\n",
      "Epoch 48/100\n",
      "750/750 [==============================] - 394s - loss: -0.4875 - dice_coef: 0.4875 - val_loss: -0.4188 - val_dice_coef: 0.4188\n",
      "Epoch 49/100\n",
      "750/750 [==============================] - 392s - loss: -0.4902 - dice_coef: 0.4902 - val_loss: -0.4439 - val_dice_coef: 0.4439\n",
      "Epoch 50/100\n",
      "750/750 [==============================] - 396s - loss: -0.5118 - dice_coef: 0.5118 - val_loss: -0.4418 - val_dice_coef: 0.4418\n",
      "Epoch 51/100\n",
      "750/750 [==============================] - 499s - loss: -0.5189 - dice_coef: 0.5189 - val_loss: -0.4312 - val_dice_coef: 0.4312\n",
      "Epoch 52/100\n",
      "750/750 [==============================] - 410s - loss: -0.5080 - dice_coef: 0.5080 - val_loss: -0.4205 - val_dice_coef: 0.4205\n",
      "Epoch 53/100\n",
      "750/750 [==============================] - 528s - loss: -0.5281 - dice_coef: 0.5281 - val_loss: -0.4703 - val_dice_coef: 0.4703\n",
      "Epoch 54/100\n",
      "750/750 [==============================] - 628s - loss: -0.5369 - dice_coef: 0.5369 - val_loss: -0.4630 - val_dice_coef: 0.4630\n",
      "Epoch 55/100\n",
      "750/750 [==============================] - 647s - loss: -0.5397 - dice_coef: 0.5397 - val_loss: -0.4756 - val_dice_coef: 0.4756\n",
      "Epoch 56/100\n",
      "750/750 [==============================] - 658s - loss: -0.5408 - dice_coef: 0.5408 - val_loss: -0.4752 - val_dice_coef: 0.4752\n",
      "Epoch 57/100\n",
      "750/750 [==============================] - 641s - loss: -0.5504 - dice_coef: 0.5504 - val_loss: -0.4809 - val_dice_coef: 0.4809\n",
      "Epoch 58/100\n",
      "750/750 [==============================] - 646s - loss: -0.5451 - dice_coef: 0.5451 - val_loss: -0.4626 - val_dice_coef: 0.4626\n",
      "Epoch 59/100\n",
      "750/750 [==============================] - 634s - loss: -0.5219 - dice_coef: 0.5219 - val_loss: -0.4761 - val_dice_coef: 0.4761\n",
      "Epoch 60/100\n",
      "750/750 [==============================] - 628s - loss: -0.5552 - dice_coef: 0.5552 - val_loss: -0.4859 - val_dice_coef: 0.4859\n",
      "Epoch 61/100\n",
      "750/750 [==============================] - 655s - loss: -0.5558 - dice_coef: 0.5558 - val_loss: -0.4465 - val_dice_coef: 0.4465\n",
      "Epoch 62/100\n",
      "750/750 [==============================] - 643s - loss: -0.5432 - dice_coef: 0.5432 - val_loss: -0.4891 - val_dice_coef: 0.4891\n",
      "Epoch 63/100\n",
      "750/750 [==============================] - 664s - loss: -0.5712 - dice_coef: 0.5712 - val_loss: -0.4922 - val_dice_coef: 0.4922\n",
      "Epoch 64/100\n",
      "750/750 [==============================] - 661s - loss: -0.5762 - dice_coef: 0.5762 - val_loss: -0.4980 - val_dice_coef: 0.4980\n",
      "Epoch 65/100\n",
      "750/750 [==============================] - 703s - loss: -0.5735 - dice_coef: 0.5735 - val_loss: -0.4766 - val_dice_coef: 0.4766\n",
      "Epoch 66/100\n",
      "750/750 [==============================] - 681s - loss: -0.5781 - dice_coef: 0.5781 - val_loss: -0.5027 - val_dice_coef: 0.5027\n",
      "Epoch 67/100\n",
      "750/750 [==============================] - 674s - loss: -0.5963 - dice_coef: 0.5963 - val_loss: -0.4966 - val_dice_coef: 0.4966\n",
      "Epoch 68/100\n",
      "750/750 [==============================] - 677s - loss: -0.5788 - dice_coef: 0.5788 - val_loss: -0.4847 - val_dice_coef: 0.4847\n",
      "Epoch 69/100\n",
      "750/750 [==============================] - 704s - loss: -0.6050 - dice_coef: 0.6050 - val_loss: -0.5076 - val_dice_coef: 0.5076\n",
      "Epoch 72/100\n",
      "750/750 [==============================] - 676s - loss: -0.6083 - dice_coef: 0.6083 - val_loss: -0.4944 - val_dice_coef: 0.4944\n",
      "Epoch 73/100\n",
      "750/750 [==============================] - 649s - loss: -0.5886 - dice_coef: 0.5886 - val_loss: -0.5082 - val_dice_coef: 0.5082\n",
      "Epoch 74/100\n",
      "750/750 [==============================] - 248s - loss: -0.6080 - dice_coef: 0.6080 - val_loss: -0.5119 - val_dice_coef: 0.5119\n",
      "Epoch 75/100\n",
      "750/750 [==============================] - 244s - loss: -0.6260 - dice_coef: 0.6260 - val_loss: -0.5127 - val_dice_coef: 0.5127\n",
      "Epoch 76/100\n",
      "750/750 [==============================] - 244s - loss: -0.6228 - dice_coef: 0.6228 - val_loss: -0.5188 - val_dice_coef: 0.5188\n",
      "Epoch 77/100\n",
      "750/750 [==============================] - 245s - loss: -0.6241 - dice_coef: 0.6241 - val_loss: -0.5151 - val_dice_coef: 0.5151\n",
      "Epoch 78/100\n",
      "750/750 [==============================] - 243s - loss: -0.6366 - dice_coef: 0.6366 - val_loss: -0.5087 - val_dice_coef: 0.5087\n",
      "Epoch 79/100\n",
      "750/750 [==============================] - 244s - loss: -0.6388 - dice_coef: 0.6388 - val_loss: -0.5152 - val_dice_coef: 0.5152\n",
      "Epoch 80/100\n",
      "750/750 [==============================] - 248s - loss: -0.6346 - dice_coef: 0.6346 - val_loss: -0.5096 - val_dice_coef: 0.5096\n",
      "Epoch 81/100\n",
      "750/750 [==============================] - 239s - loss: -0.6464 - dice_coef: 0.6464 - val_loss: -0.4870 - val_dice_coef: 0.4870\n",
      "Epoch 82/100\n",
      "750/750 [==============================] - 249s - loss: -0.6295 - dice_coef: 0.6295 - val_loss: -0.5075 - val_dice_coef: 0.5075\n",
      "Epoch 83/100\n",
      "750/750 [==============================] - 309s - loss: -0.6382 - dice_coef: 0.6382 - val_loss: -0.4930 - val_dice_coef: 0.4930\n",
      "Epoch 84/100\n",
      "750/750 [==============================] - 426s - loss: -0.6456 - dice_coef: 0.6456 - val_loss: -0.5212 - val_dice_coef: 0.5212\n",
      "Epoch 85/100\n",
      "750/750 [==============================] - 249s - loss: -0.6510 - dice_coef: 0.6510 - val_loss: -0.5269 - val_dice_coef: 0.5269\n",
      "Epoch 86/100\n",
      "750/750 [==============================] - 241s - loss: -0.6602 - dice_coef: 0.6602 - val_loss: -0.5088 - val_dice_coef: 0.5088\n",
      "Epoch 87/100\n",
      "750/750 [==============================] - 246s - loss: -0.6621 - dice_coef: 0.6621 - val_loss: -0.4889 - val_dice_coef: 0.4889\n",
      "Epoch 88/100\n",
      "750/750 [==============================] - 243s - loss: -0.6356 - dice_coef: 0.6356 - val_loss: -0.4997 - val_dice_coef: 0.4997\n",
      "Epoch 89/100\n",
      "750/750 [==============================] - 241s - loss: -0.6616 - dice_coef: 0.6616 - val_loss: -0.5171 - val_dice_coef: 0.5171\n",
      "Epoch 90/100\n",
      "750/750 [==============================] - 246s - loss: -0.6566 - dice_coef: 0.6566 - val_loss: -0.5282 - val_dice_coef: 0.5282\n",
      "Epoch 91/100\n",
      "750/750 [==============================] - 243s - loss: -0.6782 - dice_coef: 0.6782 - val_loss: -0.5219 - val_dice_coef: 0.5219\n",
      "Epoch 92/100\n",
      "750/750 [==============================] - 249s - loss: -0.6790 - dice_coef: 0.6790 - val_loss: -0.5193 - val_dice_coef: 0.5193\n",
      "Epoch 93/100\n",
      "750/750 [==============================] - 252s - loss: -0.6849 - dice_coef: 0.6849 - val_loss: -0.5180 - val_dice_coef: 0.5180\n",
      "Epoch 94/100\n",
      "750/750 [==============================] - 248s - loss: -0.6780 - dice_coef: 0.6780 - val_loss: -0.5274 - val_dice_coef: 0.5274\n",
      "Epoch 95/100\n",
      "750/750 [==============================] - 276s - loss: -0.6873 - dice_coef: 0.6873 - val_loss: -0.4681 - val_dice_coef: 0.4681\n",
      "Epoch 96/100\n",
      "750/750 [==============================] - 258s - loss: -0.6538 - dice_coef: 0.6538 - val_loss: -0.5167 - val_dice_coef: 0.5167\n",
      "Epoch 97/100\n",
      "750/750 [==============================] - 329s - loss: -0.6684 - dice_coef: 0.6684 - val_loss: -0.4878 - val_dice_coef: 0.4878\n",
      "Epoch 98/100\n",
      "750/750 [==============================] - 260s - loss: -0.6662 - dice_coef: 0.6662 - val_loss: -0.5305 - val_dice_coef: 0.5305\n",
      "Epoch 99/100\n",
      "750/750 [==============================] - 336s - loss: -0.6873 - dice_coef: 0.6873 - val_loss: -0.5171 - val_dice_coef: 0.5171\n",
      "Epoch 100/100\n",
      "750/750 [==============================] - 393s - loss: -0.6967 - dice_coef: 0.6967 - val_loss: -0.5009 - val_dice_coef: 0.5009\n",
      "------------------------------\n",
      "Loading and preprocessing test data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Loading saved weights...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "160/300 [===============>..............] - ETA: 25s"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d19bfc3e135c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimgs_mask_test_unet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_mask_test_unet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimgs_mask_test_unet\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mimgs_mask_test_unet\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimgs_mask_test_unet\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255.\u001b[0m  \u001b[0;31m# scale masks to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot\n",
    "imgs_test = np.load(\"imgs_test.npy\")\n",
    "\n",
    "imgs_mask_test_unet = np.load(\"imgs_mask_test_unet.npy\")\n",
    "\n",
    "imgs_mask_test_unet = imgs_mask_test_unet.astype('float32')\n",
    "\n",
    "imgs_mask_test_unet -= mean\n",
    "imgs_mask_test_unet /= std\n",
    "imgs_mask_test_unet /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "import pylab as pl\n",
    "for i in range(5):\n",
    "    pl.figure(figsize=(12, 2))\n",
    "    pl.subplot(142)\n",
    "    pl.title('Test')\n",
    "    pl.imshow(imgs_test[i].squeeze())\n",
    "    pl.subplot(143)\n",
    "    pl.imshow(imgs_mask_test_unet[i].squeeze())\n",
    "    pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
