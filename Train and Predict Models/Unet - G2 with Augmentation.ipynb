{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/5635.0 images\n",
      "Done: 100/5635.0 images\n",
      "Done: 200/5635.0 images\n",
      "Done: 300/5635.0 images\n",
      "Done: 400/5635.0 images\n",
      "Done: 500/5635.0 images\n",
      "Done: 600/5635.0 images\n",
      "Done: 700/5635.0 images\n",
      "Done: 800/5635.0 images\n",
      "Done: 900/5635.0 images\n",
      "Done: 1000/5635.0 images\n",
      "Done: 1100/5635.0 images\n",
      "Done: 1200/5635.0 images\n",
      "Done: 1300/5635.0 images\n",
      "Done: 1400/5635.0 images\n",
      "Done: 1500/5635.0 images\n",
      "Done: 1600/5635.0 images\n",
      "Done: 1700/5635.0 images\n",
      "Done: 1800/5635.0 images\n",
      "Done: 1900/5635.0 images\n",
      "Done: 2000/5635.0 images\n",
      "Done: 2100/5635.0 images\n",
      "Done: 2200/5635.0 images\n",
      "Done: 2300/5635.0 images\n",
      "Done: 2400/5635.0 images\n",
      "Done: 2500/5635.0 images\n",
      "Done: 2600/5635.0 images\n",
      "Done: 2700/5635.0 images\n",
      "Done: 2800/5635.0 images\n",
      "Done: 2900/5635.0 images\n",
      "Done: 3000/5635.0 images\n",
      "Done: 3100/5635.0 images\n",
      "Done: 3200/5635.0 images\n",
      "Done: 3300/5635.0 images\n",
      "Done: 3400/5635.0 images\n",
      "Done: 3500/5635.0 images\n",
      "Done: 3600/5635.0 images\n",
      "Done: 3700/5635.0 images\n",
      "Done: 3800/5635.0 images\n",
      "Done: 3900/5635.0 images\n",
      "Done: 4000/5635.0 images\n",
      "Done: 4100/5635.0 images\n",
      "Done: 4200/5635.0 images\n",
      "Done: 4300/5635.0 images\n",
      "Done: 4400/5635.0 images\n",
      "Done: 4500/5635.0 images\n",
      "Done: 4600/5635.0 images\n",
      "Done: 4700/5635.0 images\n",
      "Done: 4800/5635.0 images\n",
      "Done: 4900/5635.0 images\n",
      "Done: 5000/5635.0 images\n",
      "Done: 5100/5635.0 images\n",
      "Done: 5200/5635.0 images\n",
      "Done: 5300/5635.0 images\n",
      "Done: 5400/5635.0 images\n",
      "Done: 5500/5635.0 images\n",
      "Done: 5600/5635.0 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "Done: 0/5508 images\n",
      "Done: 100/5508 images\n",
      "Done: 200/5508 images\n",
      "Done: 300/5508 images\n",
      "Done: 400/5508 images\n",
      "Done: 500/5508 images\n",
      "Done: 600/5508 images\n",
      "Done: 700/5508 images\n",
      "Done: 800/5508 images\n",
      "Done: 900/5508 images\n",
      "Done: 1000/5508 images\n",
      "Done: 1100/5508 images\n",
      "Done: 1200/5508 images\n",
      "Done: 1300/5508 images\n",
      "Done: 1400/5508 images\n",
      "Done: 1500/5508 images\n",
      "Done: 1600/5508 images\n",
      "Done: 1700/5508 images\n",
      "Done: 1800/5508 images\n",
      "Done: 1900/5508 images\n",
      "Done: 2000/5508 images\n",
      "Done: 2100/5508 images\n",
      "Done: 2200/5508 images\n",
      "Done: 2300/5508 images\n",
      "Done: 2400/5508 images\n",
      "Done: 2500/5508 images\n",
      "Done: 2600/5508 images\n",
      "Done: 2700/5508 images\n",
      "Done: 2800/5508 images\n",
      "Done: 2900/5508 images\n",
      "Done: 3000/5508 images\n",
      "Done: 3100/5508 images\n",
      "Done: 3200/5508 images\n",
      "Done: 3300/5508 images\n",
      "Done: 3400/5508 images\n",
      "Done: 3500/5508 images\n",
      "Done: 3600/5508 images\n",
      "Done: 3700/5508 images\n",
      "Done: 3800/5508 images\n",
      "Done: 3900/5508 images\n",
      "Done: 4000/5508 images\n",
      "Done: 4100/5508 images\n",
      "Done: 4200/5508 images\n",
      "Done: 4300/5508 images\n",
      "Done: 4400/5508 images\n",
      "Done: 4500/5508 images\n",
      "Done: 4600/5508 images\n",
      "Done: 4700/5508 images\n",
      "Done: 4800/5508 images\n",
      "Done: 4900/5508 images\n",
      "Done: 5000/5508 images\n",
      "Done: 5100/5508 images\n",
      "Done: 5200/5508 images\n",
      "Done: 5300/5508 images\n",
      "Done: 5400/5508 images\n",
      "Done: 5500/5508 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "data_path = '/home/ubuntu/Notebooks/raw'\n",
    "\n",
    "image_rows = 420\n",
    "image_cols = 580\n",
    "\n",
    "\n",
    "def create_train_data():\n",
    "\n",
    "    train_data_path = os.path.join(data_path, 'train')\n",
    "    images = os.listdir(train_data_path)\n",
    "    #images = images[:1001]\n",
    "    total = len(images) / 2\n",
    "\n",
    "    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.tif'\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_train.npy', imgs)\n",
    "    np.save('imgs_mask_train.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_train = imgs_train[:1000]\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    imgs_mask_train = imgs_mask_train[:1000]\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "\n",
    "def create_test_data():\n",
    "    train_data_path = os.path.join(data_path, 'test')\n",
    "    images = os.listdir(train_data_path)\n",
    "    #images = images[:500]\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_id = np.ndarray((total, ), dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img_id = int(image_name.split('.')[0])\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = np.array([img])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_id[i] = img_id\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_test.npy', imgs)\n",
    "    np.save('imgs_id_test.npy', imgs_id)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_test = imgs_test[:300]\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    imgs_id = imgs_id[:300]\n",
    "    return imgs_test, imgs_id\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_train_data()\n",
    "    create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "data_path = '/home/ubuntu/Notebooks/raw'\n",
    "\n",
    "image_rows = 420\n",
    "image_cols = 580\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_test = imgs_test[:300]\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    imgs_id = imgs_id[:300]\n",
    "    return imgs_test, imgs_id\n",
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_train = imgs_train[:1000]\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    imgs_mask_train = imgs_mask_train[:1000]\n",
    "    return imgs_train, imgs_mask_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and create the model \n",
    "from keras import optimizers\n",
    "from keras.layers import Activation, Dense, Input, Convolution2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.layer_utils import print_summary\n",
    "\n",
    "def get_unet_aug():\n",
    "    \n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    dropout1 = Dropout(0.25)(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(dropout1)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2),dim_ordering='th')(conv1)\n",
    "    #imshow(l.activation(inpic)) \n",
    "\n",
    "    #dropout2 = Dropout(0.25)(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv2)\n",
    "\n",
    "    #dropout3 = Dropout(0.25)(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv3)\n",
    "\n",
    "    #dropout4 = Dropout(0.25)(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv4)\n",
    "\n",
    "    #dropout5 = Dropout(0.25)(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid',dim_ordering='th')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    \n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    #model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    #rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #metrics={'outbin': 'accuracy'}\n",
    "    \n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[dice_coef])\n",
    "\n",
    "    print_summary(model.layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_10 (InputLayer)            (None, 1, 64, 80)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 1, 64, 80)     0           input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_134 (Convolution2D)(None, 32, 64, 80)    320         dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_135 (Convolution2D)(None, 32, 64, 80)    9248        convolution2d_134[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_29 (MaxPooling2D)   (None, 32, 32, 40)    0           convolution2d_135[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_136 (Convolution2D)(None, 64, 32, 40)    18496       maxpooling2d_29[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_137 (Convolution2D)(None, 64, 32, 40)    36928       convolution2d_136[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_30 (MaxPooling2D)   (None, 64, 16, 20)    0           convolution2d_137[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_138 (Convolution2D)(None, 128, 16, 20)   73856       maxpooling2d_30[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_139 (Convolution2D)(None, 128, 16, 20)   147584      convolution2d_138[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_31 (MaxPooling2D)   (None, 128, 8, 10)    0           convolution2d_139[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_140 (Convolution2D)(None, 256, 8, 10)    295168      maxpooling2d_31[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_141 (Convolution2D)(None, 256, 8, 10)    590080      convolution2d_140[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_32 (MaxPooling2D)   (None, 256, 4, 5)     0           convolution2d_141[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_142 (Convolution2D)(None, 512, 4, 5)     1180160     maxpooling2d_32[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_143 (Convolution2D)(None, 512, 4, 5)     2359808     convolution2d_142[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_29 (UpSampling2D)   (None, 512, 8, 10)    0           convolution2d_143[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_29 (Merge)                 (None, 768, 8, 10)    0           upsampling2d_29[0][0]            \n",
      "                                                                   convolution2d_141[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_144 (Convolution2D)(None, 256, 8, 10)    1769728     merge_29[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_145 (Convolution2D)(None, 256, 8, 10)    590080      convolution2d_144[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_30 (UpSampling2D)   (None, 256, 16, 20)   0           convolution2d_145[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_30 (Merge)                 (None, 384, 16, 20)   0           upsampling2d_30[0][0]            \n",
      "                                                                   convolution2d_139[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_146 (Convolution2D)(None, 128, 16, 20)   442496      merge_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_147 (Convolution2D)(None, 128, 16, 20)   147584      convolution2d_146[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_31 (UpSampling2D)   (None, 128, 32, 40)   0           convolution2d_147[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_31 (Merge)                 (None, 192, 32, 40)   0           upsampling2d_31[0][0]            \n",
      "                                                                   convolution2d_137[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_148 (Convolution2D)(None, 64, 32, 40)    110656      merge_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_149 (Convolution2D)(None, 64, 32, 40)    36928       convolution2d_148[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_32 (UpSampling2D)   (None, 64, 64, 80)    0           convolution2d_149[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_32 (Merge)                 (None, 96, 64, 80)    0           upsampling2d_32[0][0]            \n",
      "                                                                   convolution2d_135[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_150 (Convolution2D)(None, 32, 64, 80)    27680       merge_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_151 (Convolution2D)(None, 32, 64, 80)    9248        convolution2d_150[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_152 (Convolution2D)(None, 1, 64, 80)     33          convolution2d_151[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 7846081\n",
      "____________________________________________________________________________________________________\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 225s - loss: -0.0238 - dice_coef: 0.0238   \n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 227s - loss: -0.0249 - dice_coef: 0.0249   \n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 226s - loss: -0.0692 - dice_coef: 0.0692   \n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 226s - loss: -0.2042 - dice_coef: 0.2042   \n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 229s - loss: -0.2107 - dice_coef: 0.2107   \n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 239s - loss: -0.2311 - dice_coef: 0.2311   \n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 266s - loss: -0.2327 - dice_coef: 0.2327   \n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 293s - loss: -0.2334 - dice_coef: 0.2334   \n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 345s - loss: -0.2416 - dice_coef: 0.2416   \n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 381s - loss: -0.2455 - dice_coef: 0.2455   \n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 375s - loss: -0.2399 - dice_coef: 0.2399   \n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 354s - loss: -0.2376 - dice_coef: 0.2376   \n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 354s - loss: -0.2439 - dice_coef: 0.2439   \n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 353s - loss: -0.2455 - dice_coef: 0.2455   \n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 358s - loss: -0.2477 - dice_coef: 0.2477   \n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 357s - loss: -0.2411 - dice_coef: 0.2411   \n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 360s - loss: -0.2431 - dice_coef: 0.2431   \n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 360s - loss: -0.2486 - dice_coef: 0.2486   \n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 359s - loss: -0.2497 - dice_coef: 0.2497   \n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 343s - loss: -0.2518 - dice_coef: 0.2518   \n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 332s - loss: -0.2530 - dice_coef: 0.2530   \n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 306s - loss: -0.2529 - dice_coef: 0.2529   \n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 316s - loss: -0.2530 - dice_coef: 0.2530   \n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 300s - loss: -0.2541 - dice_coef: 0.2541   \n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 293s - loss: -0.2618 - dice_coef: 0.2618   \n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 257s - loss: -0.2733 - dice_coef: 0.2733   \n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 248s - loss: -0.2832 - dice_coef: 0.2832   \n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 240s - loss: -0.2722 - dice_coef: 0.2722   \n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 247s - loss: -0.2909 - dice_coef: 0.2909   \n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 254s - loss: -0.3081 - dice_coef: 0.3081   \n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 260s - loss: -0.3155 - dice_coef: 0.3155   \n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 261s - loss: -0.3175 - dice_coef: 0.3175   \n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 251s - loss: -0.3288 - dice_coef: 0.3288   \n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 268s - loss: -0.3272 - dice_coef: 0.3272   \n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 268s - loss: -0.3596 - dice_coef: 0.3596   \n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 283s - loss: -0.4026 - dice_coef: 0.4026   \n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 277s - loss: -0.3888 - dice_coef: 0.3888   \n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 280s - loss: -0.4099 - dice_coef: 0.4099   \n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 278s - loss: -0.4090 - dice_coef: 0.4090   \n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 479s - loss: -0.4138 - dice_coef: 0.4138   \n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 281s - loss: -0.4307 - dice_coef: 0.4307   \n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 489s - loss: -0.4357 - dice_coef: 0.4357   \n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 524s - loss: -0.4358 - dice_coef: 0.4358   \n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 555s - loss: -0.4244 - dice_coef: 0.4244   \n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 536s - loss: -0.4546 - dice_coef: 0.4546   \n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 535s - loss: -0.4593 - dice_coef: 0.4593   \n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 543s - loss: -0.4685 - dice_coef: 0.4685   \n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 560s - loss: -0.4824 - dice_coef: 0.4824   \n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 573s - loss: -0.4712 - dice_coef: 0.4712   \n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 576s - loss: -0.4952 - dice_coef: 0.4952   \n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 567s - loss: -0.4858 - dice_coef: 0.4858   \n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 561s - loss: -0.4760 - dice_coef: 0.4760   \n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 571s - loss: -0.4851 - dice_coef: 0.4851   \n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 568s - loss: -0.4924 - dice_coef: 0.4924   \n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 571s - loss: -0.4964 - dice_coef: 0.4964   \n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 568s - loss: -0.5059 - dice_coef: 0.5059   \n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 562s - loss: -0.5136 - dice_coef: 0.5136   \n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 570s - loss: -0.5151 - dice_coef: 0.5151   \n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 573s - loss: -0.5161 - dice_coef: 0.5161   \n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 562s - loss: -0.4955 - dice_coef: 0.4955   \n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 560s - loss: -0.5440 - dice_coef: 0.5440   \n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 573s - loss: -0.5386 - dice_coef: 0.5386   \n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 579s - loss: -0.5577 - dice_coef: 0.5577   \n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 584s - loss: -0.5753 - dice_coef: 0.5753   \n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 581s - loss: -0.5764 - dice_coef: 0.5764   \n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 583s - loss: -0.5659 - dice_coef: 0.5659   \n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 587s - loss: -0.5720 - dice_coef: 0.5720   \n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 680s - loss: -0.5628 - dice_coef: 0.5628   \n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 594s - loss: -0.5771 - dice_coef: 0.5771   \n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 847s - loss: -0.5596 - dice_coef: 0.5596   \n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 887s - loss: -0.5908 - dice_coef: 0.5908   \n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 898s - loss: -0.5806 - dice_coef: 0.5806   \n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 896s - loss: -0.5933 - dice_coef: 0.5933   \n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 901s - loss: -0.5967 - dice_coef: 0.5967   \n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 901s - loss: -0.6051 - dice_coef: 0.6051   \n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 893s - loss: -0.5837 - dice_coef: 0.5837   \n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 908s - loss: -0.5758 - dice_coef: 0.5758   \n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 910s - loss: -0.5910 - dice_coef: 0.5910   \n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 898s - loss: -0.6126 - dice_coef: 0.6126   \n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 908s - loss: -0.6017 - dice_coef: 0.6017   \n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 918s - loss: -0.6147 - dice_coef: 0.6147   \n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 912s - loss: -0.6181 - dice_coef: 0.6181   \n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 905s - loss: -0.6189 - dice_coef: 0.6189   \n",
      "------------------------------\n",
      "Loading and preprocessing test data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Loading saved weights...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "300/300 [==============================] - 76s    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras import optimizers\n",
    "from keras.layers import Activation, Dense, Input, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], imgs.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i, 0] = cv2.resize(imgs[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "    return imgs_p\n",
    "\n",
    "\n",
    "#def train():\n",
    "print('-'*30)\n",
    "print('Loading and preprocessing train data...')\n",
    "print('-'*30)\n",
    "imgs_train, imgs_mask_train = load_train_data()\n",
    "\n",
    "imgs_train = preprocess(imgs_train)\n",
    "imgs_mask_train = preprocess(imgs_mask_train)\n",
    "\n",
    "imgs_train = imgs_train.astype('float32')\n",
    "mean = np.mean(imgs_train)  # mean for data centering\n",
    "std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "imgs_train -= mean\n",
    "imgs_train /= std\n",
    "\n",
    "imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "print('-'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('-'*30)\n",
    "model = get_unet()\n",
    "model_checkpoint = ModelCheckpoint('unet_augment.hdf5', monitor='loss', save_best_only=True)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "\n",
    "print('-'*30)\n",
    "print('Fitting model...')\n",
    "print('-'*30)\n",
    "#model.fit(imgs_train, imgs_mask_train, batch_size=32, nb_epoch=1, verbose=1, shuffle=True,\n",
    " #         callbacks=[model_checkpoint,csv_logger], validation_split=0.25)\n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range=5,\n",
    "            vertical_flip=True,\n",
    "            horizontal_flip=True,\n",
    "            )\n",
    "model.fit_generator(datagen.flow(imgs_train, imgs_mask_train, batch_size=32, shuffle=True),\n",
    "           samples_per_epoch=len(imgs_train),nb_epoch=100, verbose=1,callbacks=[model_checkpoint,csv_logger])\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading and preprocessing test data...')\n",
    "print('-'*30)\n",
    "imgs_test, imgs_id_test = load_test_data()\n",
    "imgs_test = preprocess(imgs_test)\n",
    "\n",
    "imgs_test = imgs_test.astype('float32')\n",
    "imgs_test -= mean\n",
    "imgs_test /= std\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading saved weights...')\n",
    "print('-'*30)\n",
    "model.load_weights('unet_augment.hdf5')\n",
    "\n",
    "print('-'*30)\n",
    "print('Predicting masks on test data...')\n",
    "print('-'*30)\n",
    "imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "np.save('imgs_mask_test_augment', imgs_mask_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs_mask_test_augment = np.load(\"imgs_mask_test_augment.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('tf')  # Tensor Flow dimension ordering in this code\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot\n",
    "imgs_test = np.load(\"imgs_test.npy\")\n",
    "\n",
    "imgs_mask_test_augment = np.load(\"imgs_mask_test_augment.npy\")\n",
    "\n",
    "imgs_mask_test_augment = imgs_mask_test_augment.astype('float32')\n",
    "\n",
    "#imgs_mask_test_augment -= mean\n",
    "#imgs_mask_test_augment /= std\n",
    "imgs_mask_test_augment /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "import pylab as pl\n",
    "for i in range(5):\n",
    "    pl.figure(figsize=(12, 2))\n",
    "    pl.subplot(142)\n",
    "    pl.title('Test')\n",
    "    pl.imshow(imgs_test[i].squeeze())\n",
    "    pl.subplot(143)\n",
    "    pl.imshow(imgs_mask_test_augment[i].squeeze())\n",
    "    pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
