{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_train = imgs_train[:1000]\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    imgs_mask_train = imgs_mask_train[:1000]\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_test = imgs_test[:300]\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    imgs_id = imgs_id[:300]\n",
    "    return imgs_test, imgs_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('tf')  # Tensor Flow dimension ordering in this code\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.layer_utils import print_summary\n",
    "\n",
    "\n",
    "def get_unet_1():\n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    dropout1 = Dropout(0.25)(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(dropout1)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2),dim_ordering='th')(conv1)\n",
    "    #imshow(l.activation(inpic)) \n",
    "\n",
    "    #dropout2 = Dropout(0.25)(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv2)\n",
    "\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv2), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid',dim_ordering='th')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    #model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    #rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=[dice_coef])\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    #model.compile(optimizer='adam', loss=dice_coef_loss, metric='accuracy',dim_ordering=\"th\")\n",
    "    #metrics={'outbin': 'accuracy'}\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    print_summary(model.layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], imgs.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i, 0] = cv2.resize(imgs[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "    return imgs_p\n",
    "\n",
    "\n",
    "def train_and_predict():\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "\n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet_1()\n",
    "    model_checkpoint = ModelCheckpoint('model1.hdf5', monitor='loss', save_best_only=True)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=32, nb_epoch=50, verbose=1, shuffle=True,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing test data...')\n",
    "    print('-'*30)\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('model1.hdf5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "    np.save('imgs_mask_test_model1.npy', imgs_mask_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading saved weights...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "224/300 [=====================>........] - ETA: 9s "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bf74a1634757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicting masks on test data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mimgs_mask_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imgs_mask_test.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs_mask_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1197\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "imgs_test, imgs_id_test = load_test_data()\n",
    "imgs_test = preprocess(imgs_test)\n",
    "\n",
    "\n",
    "\n",
    "print('-'*30)\n",
    "print('Loading saved weights...')\n",
    "print('-'*30)\n",
    "model.load_weights('model1.hdf5')\n",
    "\n",
    "print('-'*30)\n",
    "print('Predicting masks on test data...')\n",
    "print('-'*30)\n",
    "imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "np.save('imgs_mask_test.npy', imgs_mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 1, 64, 80)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 1, 64, 80)     0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 32, 64, 80)    320         dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 32, 64, 80)    9248        convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 32, 32, 40)    0           convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_17 (Convolution2D) (None, 64, 32, 40)    18496       maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_18 (Convolution2D) (None, 64, 32, 40)    36928       convolution2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_3 (UpSampling2D)    (None, 64, 64, 80)    0           convolution2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 96, 64, 80)    0           upsampling2d_3[0][0]             \n",
      "                                                                   convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_19 (Convolution2D) (None, 32, 64, 80)    27680       merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_20 (Convolution2D) (None, 32, 64, 80)    9248        convolution2d_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 1, 64, 80)     33          convolution2d_20[0][0]           \n",
      "====================================================================================================\n",
      "Total params: 101953\n",
      "____________________________________________________________________________________________________\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 267s - loss: -0.0234 - dice_coef: 0.0234   \n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 269s - loss: -0.0239 - dice_coef: 0.0239   \n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 268s - loss: -0.0244 - dice_coef: 0.0244   \n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.0251 - dice_coef: 0.0251   \n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.0261 - dice_coef: 0.0261   \n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 270s - loss: -0.0273 - dice_coef: 0.0273   \n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.0290 - dice_coef: 0.0290   \n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 270s - loss: -0.0310 - dice_coef: 0.0310   \n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.0330 - dice_coef: 0.0330   \n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 270s - loss: -0.0352 - dice_coef: 0.0352   \n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 270s - loss: -0.0381 - dice_coef: 0.0381   \n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 270s - loss: -0.0425 - dice_coef: 0.0425   \n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 270s - loss: -0.0489 - dice_coef: 0.0489   \n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.0560 - dice_coef: 0.0560   \n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.0660 - dice_coef: 0.0660   \n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.0771 - dice_coef: 0.0771   \n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.0899 - dice_coef: 0.0899   \n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1014 - dice_coef: 0.1014   \n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1109 - dice_coef: 0.1109   \n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.1201 - dice_coef: 0.1201   \n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.1297 - dice_coef: 0.1297   \n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1385 - dice_coef: 0.1385   \n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 273s - loss: -0.1482 - dice_coef: 0.1482   \n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1531 - dice_coef: 0.1531   \n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.1591 - dice_coef: 0.1591   \n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 273s - loss: -0.1661 - dice_coef: 0.1661   \n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 273s - loss: -0.1731 - dice_coef: 0.1731   \n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 274s - loss: -0.1780 - dice_coef: 0.1780   \n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1802 - dice_coef: 0.1802   \n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 273s - loss: -0.1826 - dice_coef: 0.1826   \n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 274s - loss: -0.1861 - dice_coef: 0.1861   \n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1906 - dice_coef: 0.1906   \n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1972 - dice_coef: 0.1972   \n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.1940 - dice_coef: 0.1940   \n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 274s - loss: -0.1983 - dice_coef: 0.1983   \n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.2034 - dice_coef: 0.2034   \n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.2096 - dice_coef: 0.2096   \n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 273s - loss: -0.2090 - dice_coef: 0.2090   \n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 274s - loss: -0.2077 - dice_coef: 0.2077   \n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.2137 - dice_coef: 0.2137   \n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 273s - loss: -0.2181 - dice_coef: 0.2181   \n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 271s - loss: -0.2181 - dice_coef: 0.2181   \n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 272s - loss: -0.2239 - dice_coef: 0.2239   \n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 274s - loss: -0.2219 - dice_coef: 0.2219   \n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 274s - loss: -0.2293 - dice_coef: 0.2293   \n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 230s - loss: -0.2269 - dice_coef: 0.2269   \n",
      "------------------------------\n",
      "Loading and preprocessing test data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Loading saved weights...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "300/300 [==============================] - 24s    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_predict()\n",
    "#keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot\n",
    "imgs_test = np.load(\"imgs_test.npy\")\n",
    "\n",
    "imgs_mask_test_augment = np.load(\"imgs_mask_test_model1.npy\")\n",
    "\n",
    "imgs_mask_test_augment = imgs_mask_test_augment.astype('float32')\n",
    "\n",
    "#imgs_mask_test_augment -= mean\n",
    "#imgs_mask_test_augment /= std\n",
    "imgs_mask_test_augment /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "import pylab as pl\n",
    "for i in range(5):\n",
    "    pl.figure(figsize=(12, 2))\n",
    "    pl.subplot(142)\n",
    "    pl.title('Test')\n",
    "    pl.imshow(imgs_test[i].squeeze())\n",
    "    pl.subplot(143)\n",
    "    pl.imshow(imgs_mask_test_augment[i].squeeze())\n",
    "    pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
