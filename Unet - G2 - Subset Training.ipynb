{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:18: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:19: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/5635.0 images\n",
      "Done: 100/5635.0 images\n",
      "Done: 200/5635.0 images\n",
      "Done: 300/5635.0 images\n",
      "Done: 400/5635.0 images\n",
      "Done: 500/5635.0 images\n",
      "Done: 600/5635.0 images\n",
      "Done: 700/5635.0 images\n",
      "Done: 800/5635.0 images\n",
      "Done: 900/5635.0 images\n",
      "Done: 1000/5635.0 images\n",
      "Done: 1100/5635.0 images\n",
      "Done: 1200/5635.0 images\n",
      "Done: 1300/5635.0 images\n",
      "Done: 1400/5635.0 images\n",
      "Done: 1500/5635.0 images\n",
      "Done: 1600/5635.0 images\n",
      "Done: 1700/5635.0 images\n",
      "Done: 1800/5635.0 images\n",
      "Done: 1900/5635.0 images\n",
      "Done: 2000/5635.0 images\n",
      "Done: 2100/5635.0 images\n",
      "Done: 2200/5635.0 images\n",
      "Done: 2300/5635.0 images\n",
      "Done: 2400/5635.0 images\n",
      "Done: 2500/5635.0 images\n",
      "Done: 2600/5635.0 images\n",
      "Done: 2700/5635.0 images\n",
      "Done: 2800/5635.0 images\n",
      "Done: 2900/5635.0 images\n",
      "Done: 3000/5635.0 images\n",
      "Done: 3100/5635.0 images\n",
      "Done: 3200/5635.0 images\n",
      "Done: 3300/5635.0 images\n",
      "Done: 3400/5635.0 images\n",
      "Done: 3500/5635.0 images\n",
      "Done: 3600/5635.0 images\n",
      "Done: 3700/5635.0 images\n",
      "Done: 3800/5635.0 images\n",
      "Done: 3900/5635.0 images\n",
      "Done: 4000/5635.0 images\n",
      "Done: 4100/5635.0 images\n",
      "Done: 4200/5635.0 images\n",
      "Done: 4300/5635.0 images\n",
      "Done: 4400/5635.0 images\n",
      "Done: 4500/5635.0 images\n",
      "Done: 4600/5635.0 images\n",
      "Done: 4700/5635.0 images\n",
      "Done: 4800/5635.0 images\n",
      "Done: 4900/5635.0 images\n",
      "Done: 5000/5635.0 images\n",
      "Done: 5100/5635.0 images\n",
      "Done: 5200/5635.0 images\n",
      "Done: 5300/5635.0 images\n",
      "Done: 5400/5635.0 images\n",
      "Done: 5500/5635.0 images\n",
      "Done: 5600/5635.0 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "Done: 0/5508 images\n",
      "Done: 100/5508 images\n",
      "Done: 200/5508 images\n",
      "Done: 300/5508 images\n",
      "Done: 400/5508 images\n",
      "Done: 500/5508 images\n",
      "Done: 600/5508 images\n",
      "Done: 700/5508 images\n",
      "Done: 800/5508 images\n",
      "Done: 900/5508 images\n",
      "Done: 1000/5508 images\n",
      "Done: 1100/5508 images\n",
      "Done: 1200/5508 images\n",
      "Done: 1300/5508 images\n",
      "Done: 1400/5508 images\n",
      "Done: 1500/5508 images\n",
      "Done: 1600/5508 images\n",
      "Done: 1700/5508 images\n",
      "Done: 1800/5508 images\n",
      "Done: 1900/5508 images\n",
      "Done: 2000/5508 images\n",
      "Done: 2100/5508 images\n",
      "Done: 2200/5508 images\n",
      "Done: 2300/5508 images\n",
      "Done: 2400/5508 images\n",
      "Done: 2500/5508 images\n",
      "Done: 2600/5508 images\n",
      "Done: 2700/5508 images\n",
      "Done: 2800/5508 images\n",
      "Done: 2900/5508 images\n",
      "Done: 3000/5508 images\n",
      "Done: 3100/5508 images\n",
      "Done: 3200/5508 images\n",
      "Done: 3300/5508 images\n",
      "Done: 3400/5508 images\n",
      "Done: 3500/5508 images\n",
      "Done: 3600/5508 images\n",
      "Done: 3700/5508 images\n",
      "Done: 3800/5508 images\n",
      "Done: 3900/5508 images\n",
      "Done: 4000/5508 images\n",
      "Done: 4100/5508 images\n",
      "Done: 4200/5508 images\n",
      "Done: 4300/5508 images\n",
      "Done: 4400/5508 images\n",
      "Done: 4500/5508 images\n",
      "Done: 4600/5508 images\n",
      "Done: 4700/5508 images\n",
      "Done: 4800/5508 images\n",
      "Done: 4900/5508 images\n",
      "Done: 5000/5508 images\n",
      "Done: 5100/5508 images\n",
      "Done: 5200/5508 images\n",
      "Done: 5300/5508 images\n",
      "Done: 5400/5508 images\n",
      "Done: 5500/5508 images\n",
      "Loading done.\n",
      "Saving to .npy files done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "data_path = '/home/ubuntu/Notebooks/raw'\n",
    "\n",
    "image_rows = 420\n",
    "image_cols = 580\n",
    "\n",
    "\n",
    "def create_train_data():\n",
    "\n",
    "    train_data_path = os.path.join(data_path, 'train')\n",
    "    images = os.listdir(train_data_path)\n",
    "    #images = images[:1001]\n",
    "    total = len(images) / 2\n",
    "\n",
    "    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.tif'\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_train.npy', imgs)\n",
    "    np.save('imgs_mask_train.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    imgs_train = np.load('imgs_train.npy')\n",
    "    imgs_train = imgs_train[:1000]\n",
    "    imgs_mask_train = np.load('imgs_mask_train.npy')\n",
    "    imgs_mask_train = imgs_mask_train[:1000]\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "\n",
    "def create_test_data():\n",
    "    train_data_path = os.path.join(data_path, 'test')\n",
    "    images = os.listdir(train_data_path)\n",
    "    #images = images[:500]\n",
    "    total = len(images)\n",
    "\n",
    "    imgs = np.ndarray((total, 1, image_rows, image_cols), dtype=np.uint8)\n",
    "    imgs_id = np.ndarray((total, ), dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        img_id = int(image_name.split('.')[0])\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img = np.array([img])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_id[i] = img_id\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save('imgs_test.npy', imgs)\n",
    "    np.save('imgs_id_test.npy', imgs_id)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    imgs_test = np.load('imgs_test.npy')\n",
    "    imgs_test = imgs_test[:300]\n",
    "    imgs_id = np.load('imgs_id_test.npy')\n",
    "    imgs_id = imgs_id_test[:300]\n",
    "    return imgs_test, imgs_id\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_train_data()\n",
    "    create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and create the model \n",
    "from keras import optimizers\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "def get_unet():\n",
    "    from keras.utils.layer_utils import print_summary\n",
    "    inputs = Input((1, img_rows, img_cols))\n",
    "    dropout1 = Dropout(0.25)(inputs)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(dropout1)\n",
    "    conv1 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2),dim_ordering='th')(conv1)\n",
    "    #imshow(l.activation(inpic)) \n",
    "\n",
    "    #dropout2 = Dropout(0.25)(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool1)\n",
    "    conv2 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv2)\n",
    "\n",
    "    #dropout3 = Dropout(0.25)(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool2)\n",
    "    conv3 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv3)\n",
    "\n",
    "    #dropout4 = Dropout(0.25)(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool3)\n",
    "    conv4 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\")(conv4)\n",
    "\n",
    "    #dropout5 = Dropout(0.25)(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(pool4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up6)\n",
    "    conv6 = Convolution2D(256, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up7)\n",
    "    conv7 = Convolution2D(128, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up8)\n",
    "    conv8 = Convolution2D(64, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2),dim_ordering='th')(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(up9)\n",
    "    conv9 = Convolution2D(32, 3, 3, activation='relu', border_mode='same',dim_ordering='th')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid',dim_ordering='th')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    \n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    #model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    #rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    #model.compile(optimizer=rmsprop, loss='binary_crossentropy')\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    #metrics={'outbin': 'accuracy'}\n",
    "    \n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[dice_coef])\n",
    "\n",
    "    print_summary(model.layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs_train, imgs_mask_train = load_train_data()\n",
    "print(imgs_train.shape[0])\n",
    "print(imgs_mask_train.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], imgs.shape[1], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i, 0] = cv2.resize(imgs[i, 0], (img_cols, img_rows), interpolation=cv2.INTER_CUBIC)\n",
    "    return imgs_p\n",
    "\n",
    "\n",
    "def train_and_predict():\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "\n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "    print('-'*30)\n",
    "    print('Shape')\n",
    "    print('-'*30)\n",
    "    \n",
    "    \n",
    "    \n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet()\n",
    "    model_checkpoint = ModelCheckpoint('unet.hdf5_unet', monitor='loss', save_best_only=True)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=32, nb_epoch=500, verbose=1, shuffle=True,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing test data...')\n",
    "    print('-'*30)\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('unet.hdf5_unet')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "    np.save('imgs_mask_test_unet.npy', imgs_mask_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('tf')  # Tensor Flow dimension ordering in this code\n",
    "img_rows = 64\n",
    "img_cols = 80\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Shape\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 1, 64, 80)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 1, 64, 80)     0           input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_96 (Convolution2D) (None, 32, 64, 80)    320         dropout_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_97 (Convolution2D) (None, 32, 64, 80)    9248        convolution2d_96[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_21 (MaxPooling2D)   (None, 32, 32, 40)    0           convolution2d_97[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_98 (Convolution2D) (None, 64, 32, 40)    18496       maxpooling2d_21[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_99 (Convolution2D) (None, 64, 32, 40)    36928       convolution2d_98[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_22 (MaxPooling2D)   (None, 64, 16, 20)    0           convolution2d_99[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_100 (Convolution2D)(None, 128, 16, 20)   73856       maxpooling2d_22[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_101 (Convolution2D)(None, 128, 16, 20)   147584      convolution2d_100[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_23 (MaxPooling2D)   (None, 128, 8, 10)    0           convolution2d_101[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_102 (Convolution2D)(None, 256, 8, 10)    295168      maxpooling2d_23[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_103 (Convolution2D)(None, 256, 8, 10)    590080      convolution2d_102[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_24 (MaxPooling2D)   (None, 256, 4, 5)     0           convolution2d_103[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_104 (Convolution2D)(None, 512, 4, 5)     1180160     maxpooling2d_24[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_105 (Convolution2D)(None, 512, 4, 5)     2359808     convolution2d_104[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_21 (UpSampling2D)   (None, 512, 8, 10)    0           convolution2d_105[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_21 (Merge)                 (None, 768, 8, 10)    0           upsampling2d_21[0][0]            \n",
      "                                                                   convolution2d_103[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_106 (Convolution2D)(None, 256, 8, 10)    1769728     merge_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_107 (Convolution2D)(None, 256, 8, 10)    590080      convolution2d_106[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_22 (UpSampling2D)   (None, 256, 16, 20)   0           convolution2d_107[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_22 (Merge)                 (None, 384, 16, 20)   0           upsampling2d_22[0][0]            \n",
      "                                                                   convolution2d_101[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_108 (Convolution2D)(None, 128, 16, 20)   442496      merge_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_109 (Convolution2D)(None, 128, 16, 20)   147584      convolution2d_108[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_23 (UpSampling2D)   (None, 128, 32, 40)   0           convolution2d_109[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_23 (Merge)                 (None, 192, 32, 40)   0           upsampling2d_23[0][0]            \n",
      "                                                                   convolution2d_99[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_110 (Convolution2D)(None, 64, 32, 40)    110656      merge_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_111 (Convolution2D)(None, 64, 32, 40)    36928       convolution2d_110[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_24 (UpSampling2D)   (None, 64, 64, 80)    0           convolution2d_111[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_24 (Merge)                 (None, 96, 64, 80)    0           upsampling2d_24[0][0]            \n",
      "                                                                   convolution2d_97[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_112 (Convolution2D)(None, 32, 64, 80)    27680       merge_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_113 (Convolution2D)(None, 32, 64, 80)    9248        convolution2d_112[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_114 (Convolution2D)(None, 1, 64, 80)     33          convolution2d_113[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 7846081\n",
      "____________________________________________________________________________________________________\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/500\n",
      "1000/1000 [==============================] - 220s - loss: -0.0240 - dice_coef: 0.0240   \n",
      "Epoch 2/500\n",
      "1000/1000 [==============================] - 223s - loss: -0.2196 - dice_coef: 0.2196   \n",
      "Epoch 6/500\n",
      " 672/1000 [===================>..........] - ETA: 73s - loss: -0.2245 - dice_coef: 0.2245"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "train_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_mask_test = np.load(\"imgs_mask_test.npy\")\n",
    "imgs_test = np.load(\"imgs_test.npy\")\n",
    "img_mask_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
